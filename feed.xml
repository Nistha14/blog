<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Madhukar's Blog</title>
    <description>Thoughts on technology, life and everything else.</description>
    <link>http://blog.madhukaraphatak.com/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Migrating to Spark 2.0 - Part 5 : Meetup Talk</title>
        <description>&lt;p&gt;In last few blog posts we have discussed about different aspects of the migrating to spark 2.0. 
Recently I gave a talk on these topics in a our spark &lt;a href=&quot;https://www.meetup.com/Bangalore-Apache-Spark-Meetup/&quot;&gt;meetup&lt;/a&gt;. It was first part
of two part series.&lt;/p&gt;

&lt;p&gt;In this fifth blog of the series, I will be sharing recording of that talk with slides and code.
You can access all the posts in the series &lt;a href=&quot;/categories/spark-two-migration-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Find the slides on &lt;a href=&quot;https://www.slideshare.net/datamantra/migrating-to-spark-20&quot;&gt;slideshare&lt;/a&gt; and code on &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/jyXEUXCYGwo&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

</description>
        <pubDate>Wed, 07 Jun 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/migrating-to-spark-two-part-5</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/migrating-to-spark-two-part-5</guid>
      </item>
    
      <item>
        <title>Migrating to Spark 2.0 - Part 4 : Cross Joins</title>
        <description>&lt;p&gt;Spark 2.0 brings a significant changes to abstractions and APIâ€™s of spark platform. With performance boost, this version has made some of non backward compatible changes to the framework. To keep up to date with the latest updates, one need to migrate their spark 1.x code base to 2.x. In last few weeks, I was involved in migrating one of fairly large code base and found it quite involving process. In this series of posts, I will be documenting my experience of migration so it may help all the ones out there who are planning to do the same.&lt;/p&gt;

&lt;p&gt;This is the forth post in this series.In this post we discuss about cross joins. You can access all the posts &lt;a href=&quot;/categories/spark-two-migration-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR You can access all the code on &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;joins-in-spark-sql&quot;&gt;Joins in Spark SQL&lt;/h2&gt;
&lt;p&gt;Joins are one of the costliest operations in spark or big data in general. So whenever we program in spark we try to avoid joins or restrict the joins on limited data.There are various optimisations in spark , right from choosing right type of joins and using broadcast joins to improve the performance.&lt;/p&gt;

&lt;h2 id=&quot;cross-joins&quot;&gt;Cross Joins&lt;/h2&gt;

&lt;p&gt;Cross Join or cartesian product is one kind of join where each row of one dataset is joined with other. So if we have a dataset of size m and if we join with other dataset with of size n , we will getting a dataset with m*n number of rows.&lt;/p&gt;

&lt;p&gt;Cross joins are one of the most time consuming joins and often should be avoided. But sometimes, we may accidentally do them without intending to do so. But we recognise performance issue only when they run on large data. So having ability to identify them at earliest will save lot of hassle.&lt;/p&gt;

&lt;h2 id=&quot;cross-joins-in-1x&quot;&gt;Cross Joins in 1.x&lt;/h2&gt;

&lt;p&gt;The below code is an example of cross join in 1.x. In this example, we are doing a self join without any condition.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDf&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;com.databricks.spark.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;header&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;../test_data/sales.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

 &lt;span class=&quot;c1&quot;&gt;//cross join the data&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;crossJoinDf&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loadedDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// count the joined df&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;crossJoinDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explain&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;())&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;crossJoinDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The plan looks as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;nc&quot;&gt;CartesianProduct&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;:-&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Scan&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CsvRelation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Some&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(../&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sales&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;,,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;,null,#,PERMISSIVE,COMMONS,false,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;false,false,null,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;false,null,,null,100000)[transactionId#0,&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;customerId#1,itemId#2,amountPaid#3]&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;+- Scan CsvRelation(&amp;lt;function0&amp;gt;,Some(../test_data/sales.csv),&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;true,,,&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,#,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;PERMISSIVE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;COMMONS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,,&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;transactionId&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;9&lt;/span&gt;,&lt;span class=&quot;kt&quot;&gt;customerId&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;10&lt;/span&gt;,&lt;span class=&quot;kt&quot;&gt;itemId&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;11&lt;/span&gt;,&lt;span class=&quot;kt&quot;&gt;amountPaid&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When you observe the plan, it indicates the cartesian product.&lt;/p&gt;

&lt;p&gt;You can access complete code on &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration/blob/master/spark-one/src/main/scala/com/madhukaraphatak/spark/migration/sparkone/CrossJoin.scala&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;cross-join-in-2x&quot;&gt;Cross Join in 2.x&lt;/h2&gt;

&lt;p&gt;In 2.x, spark has &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-17298&quot;&gt;added&lt;/a&gt; an expilict check for cartersian product. By default all the joins reject the cross product. So if you run the same code in 2.x, you will get below error.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;org.apache.spark.sql.AnalysisException: Detected cartesian product for INNER join between logical plans&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So this makes sures that by accident we donâ€™t introduce any cartesian products in our code.&lt;/p&gt;

&lt;h2 id=&quot;explicit-cross-join-in-2x&quot;&gt;Explicit Cross Join in 2.x&lt;/h2&gt;

&lt;p&gt;So if you really want to have cross join, you need to be explicit in the code&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;crossJoinDf&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;crossJoin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loadedDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By explicitly specifying the cross join, spark will allow user to do cross join. This helps programmer to avoid introducing cross join accidentally.&lt;/p&gt;

&lt;p&gt;You can access complete code on &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration/blob/master/spark-two/src/main/scala/com/madhukaraphatak/spark/migration/sparktwo/CrossJoin.scala&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Spark 2.x makes sure that we donâ€™t introduce cross join accidentally. This smartness built into analyser helps to improve the performance of many workloads.&lt;/p&gt;
</description>
        <pubDate>Wed, 10 May 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/migrating-to-spark-two-part-4</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/migrating-to-spark-two-part-4</guid>
      </item>
    
      <item>
        <title>Migrating to Spark 2.0 - Part 3 : DataFrame to Dataset</title>
        <description>&lt;p&gt;Spark 2.0 brings a significant changes to abstractions and APIâ€™s of spark platform. With performance boost, this version has made some of non backward compatible changes to the framework. To keep up to date with the latest updates, one need to migrate their spark 1.x code base to 2.x. In last few weeks, I was involved in migrating one of fairly large code base and found it quite involving process. In this series of posts, I will be documenting my experience of migration so it may help all the ones out there who are planning to do the same.&lt;/p&gt;

&lt;p&gt;This is the third post in this series.In this post we discuss about migrating  dataframe based APIâ€™s to dataset based once. You can access all the posts &lt;a href=&quot;/categories/spark-two-migration-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR You can access all the code on &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;dataframe-abstraction-in-spark-1x&quot;&gt;DataFrame Abstraction in Spark 1.x&lt;/h2&gt;

&lt;p&gt;In 1.x series of spark, dataframe was a structured abstraction over native RDD abstraction. Dataframe 
was built to support the structured transformation of data, using dataframe dsl or spark sql query language.
Where as rdd abstraction was there to provide the functional APIâ€™s for manipulating the data.&lt;/p&gt;

&lt;p&gt;So whenever we wanted to use functional APIâ€™s on dataframe, we would be converting dataframe into a
RDD and then manipulated as RDD abstraction. This kind of conversion made itâ€™s easy to move between dataframe 
and rdd abstractions.&lt;/p&gt;

&lt;h2 id=&quot;dataframe-to-rdd-example&quot;&gt;DataFrame to RDD Example&lt;/h2&gt;

&lt;p&gt;The below code shows, how to use functional &lt;em&gt;map&lt;/em&gt; API on dataframe abstraction&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDF&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;com.databricks.spark.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;header&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;inferSchema&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;../test_data/sales.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;amountRDD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;â‡’&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above code, whenever we call &lt;em&gt;map&lt;/em&gt; spark implicitly converts to an RDD. So this is the way we operated on the dataframe in spark 1.x.&lt;/p&gt;

&lt;p&gt;You can access complete code on &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration/blob/master/spark-one/src/main/scala/com/madhukaraphatak/spark/migration/sparkone/DFMapExample.scala&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;dataframe-abstraction-in-2x&quot;&gt;DataFrame abstraction in 2.x&lt;/h2&gt;

&lt;p&gt;From spark 2.x, the dataframe abstraction has changed significantly. In 2.x, dataframe is alias of Dataset[Row].Dataset is combination of both dataframe and RDD like APIâ€™s. So not only dataset supports structured querying using dsl and sql, it also supports the functional APIâ€™s which are supported in RDD.&lt;/p&gt;

&lt;p&gt;So whenever we call &lt;em&gt;map&lt;/em&gt; in 2.x, we no more get a RDD. Instead we get dataset. This change in the conversion will break your code if you are using the RDD based code showed earlier.&lt;/p&gt;

&lt;h2 id=&quot;porting-dataframe-to-rdd-code-in-2x&quot;&gt;Porting DataFrame to RDD code in 2.x&lt;/h2&gt;

&lt;p&gt;To port your existing code,you need to add extra, &lt;em&gt;.rdd&lt;/em&gt; before calling map method as shown in the code.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;amountRDD&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;â‡’&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amountRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toList&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;rdd-to-dataset-abstraction&quot;&gt;RDD to Dataset Abstraction&lt;/h2&gt;

&lt;p&gt;The above code makes it easy to port but doesnâ€™t provide good performance. Also converting rdd back to dataframe is a tedious work. So rather than using rdd functional APIâ€™s, you can use dataset functional APIâ€™s. Only constraint is that the return type of map should have a encoder. By default all primitive types and case classes are supported.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sparkSession.implicits._&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;amountDataSet&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;â‡’&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getDouble&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By importing &lt;em&gt;sparkSession.implicits._&lt;/em&gt; in to the scope, we are importing all default encoders. As we are returning a double value, there is a built in encoder for the same.&lt;/p&gt;

&lt;p&gt;This code is more performant than the RDD code. So use dataset based functional API wherever you were using RDD before. Only fall back to RDD API, whenever dataset API doesnâ€™t support that API.&lt;/p&gt;

&lt;p&gt;You can access complete code on &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration/blob/master/spark-two/src/main/scala/com/madhukaraphatak/spark/migration/sparktwo/DFMapExample.scala&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post we learnt how to port RDD based functional APIâ€™s to more peformant dataset alternatives.&lt;/p&gt;

</description>
        <pubDate>Mon, 08 May 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/migrating-to-spark-two-part-3</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/migrating-to-spark-two-part-3</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 9 : Service Update and Rollback</title>
        <description>&lt;p&gt;In last few blog posts on kubernetes, we have discussed about how to build and scale spark cluster. Once services are deployed, we also
need to update services time to time. When we update a service, we need to make sure that, it donâ€™t interrupt the working of other
services. Kubernetes has built in support for the service update and rollbacks. This makes changing services on kubernetes
much easier than doing them manually in other platforms.&lt;/p&gt;

&lt;p&gt;In this ninth blog of the series, I will be discussing about service update and rollback in kubernetes.
You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;updating-service&quot;&gt;Updating Service&lt;/h2&gt;

&lt;p&gt;In our discussion of deployment abstraction, I told that deployment helps us to handle life cycle of a service. Currently,
we are running the spark version 2.1.0. Letâ€™s say we want to change it to 1.6.3 without changing the 
configuration. We can use deployment abstraction for achieving the same.&lt;/p&gt;

&lt;p&gt;The below are the steps to change spark version from 2.1.0 to 1.6.3 using deployment abstraction.&lt;/p&gt;

&lt;h3 id=&quot;generate-spark-image-for-163&quot;&gt;1. Generate Spark Image for 1.6.3&lt;/h3&gt;

&lt;p&gt;As we did for spark 2.1.0, we first need to have an image of the spark 1.6.3. This can be easily done by changing docker file as below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ENV spark_ver 1.6.3&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once we update the docker file, we need to build new image with below command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;docker build -t spark-1.6.3-bin-hadoop2.6 .&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have  our new spark image ready.&lt;/p&gt;

&lt;h3 id=&quot;set-images-to-deployment&quot;&gt;2. Set Images to Deployment&lt;/h3&gt;

&lt;p&gt;The below two commands sets new images to already running spark-master and spark-worker deployments&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;image deployment/spark-master spark-master&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;spark-1.6.3-bin-hadoop2.6

kubectl &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;image deployment/spark-worker spark-worker&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;spark-1.6.3-bin-hadoop2.6&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the command, we are setting the image for &lt;em&gt;spark-master&lt;/em&gt;  and &lt;em&gt;spark-worker&lt;/em&gt; container inside the deployment. This helps only update needed containers
inside deployment rather than updating all.&lt;/p&gt;

&lt;p&gt;This only sets new images. It has not updated the service yet. We need to use roll out command for that.&lt;/p&gt;

&lt;h3 id=&quot;rollout-the-deployment&quot;&gt;3. Rollout the Deployment&lt;/h3&gt;

&lt;p&gt;The below commands rolls out the changes to deployment.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl rollout status deployment/spark-master
kubectl rollout status deployment/spark-worker&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When you roll out changes, kubernetes first brings up new pods with 1.6.3 version. Then once they are running
the old pods will be deleted.&lt;/p&gt;

&lt;p&gt;This shown in the below output.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;NAME                               READY     STATUS        RESTARTS   AGE
nginx-deployment-619952658-16z1h   1/1       Running       &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;          11d
spark-master-1095292607-mb0xh      1/1       Running       &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;          37s
spark-worker-1610799992-4f701      0/1       Pending       &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;          25s
spark-worker-671341425-xxlxn       1/1       Terminating   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;          53s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As part of the roll out, kubernetes will update all dns entries to point to new pods.&lt;/p&gt;

&lt;p&gt;This graceful switch over from older version to new version of pods makes sures that service is least interrupted when we
update services.&lt;/p&gt;

&lt;p&gt;You can verify the spark version using &lt;em&gt;spark-ui&lt;/em&gt; or logging into one of the pods.&lt;/p&gt;

&lt;h2 id=&quot;service-rollback&quot;&gt;Service Rollback&lt;/h2&gt;

&lt;p&gt;As part of the service update, kubernetes remembers state of last two deployments. This helps us to roll back the changes
we made it to deployment using &lt;em&gt;undo&lt;/em&gt; command.&lt;/p&gt;

&lt;p&gt;If we want to undo our change of spark version, we can run the below commands&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl rollout undo deployment/spark-master
kubectl rollout undo deployment/spark-worker&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above commands reverses the spark version back to 2.1.0. This ability to quickly undo the service is
powerful. If something goes wrong, we can rollback the service to itâ€™s previous state without much effort.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Kubernetes has native support for service update and rollback. Using deployment abstraction we can easily roll out the changes to
our services without effecting other services.&lt;/p&gt;

</description>
        <pubDate>Wed, 03 May 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-9</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-9</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 8 : Meetup Talk</title>
        <description>&lt;p&gt;In last few blog posts on kubernetes, we have discussed how to build scalable spark cluster on kubernetes platform. 
Recently I gave a talk on these topics in a our spark &lt;a href=&quot;https://www.meetup.com/Bangalore-Apache-Spark-Meetup/&quot;&gt;meetup&lt;/a&gt;. It was focused on importance of micro
service architecture for big data development and deployment using kubernetes.&lt;/p&gt;

&lt;p&gt;In this eighth blog of the series, I will be sharing recording of that talk with slides and code.
You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Find the slides on &lt;a href=&quot;https://www.slideshare.net/datamantra/scalable-spark-deployment-using-kubernetes&quot;&gt;slideshare&lt;/a&gt; and code on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt; &lt;iframe src=&quot;https://www.youtube.com/embed/Q0miRvKA4yk&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;

</description>
        <pubDate>Tue, 02 May 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-8</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-8</guid>
      </item>
    
      <item>
        <title>Migrating to Spark 2.0 - Part 2 : Built-in CSV Connector</title>
        <description>&lt;p&gt;Spark 2.0 brings a significant changes to abstractions and APIâ€™s of spark platform. With performance boost, this version has made some of non backward compatible changes to the framework. To keep up to date with the latest updates, one need to migrate their spark 1.x code base to 2.x. In last few weeks, I was involved in migrating one of fairly large code base and found it quite involving process. In this series of posts, I will be documenting my experience of migration so it may help all the ones out there who are planning to do the same.&lt;/p&gt;

&lt;p&gt;This is the second post in this series. In this post, we will discuss about built-in csv connector. You can access all the posts &lt;a href=&quot;/categories/spark-two-migration-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR You can access all the code on &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;csv-source-connector-in-spark&quot;&gt;CSV Source Connector in Spark&lt;/h2&gt;

&lt;p&gt;In spark 1.x, csv connector was provided using, &lt;a href=&quot;/analysing-csv-data-in-spark&quot;&gt;spark-csv&lt;/a&gt;, a third party library  by databricks. But in spark 2.0, they have made csv a built-in source. This decision is primarily driven by the fact that csv is one of the major data formats used in enterprises.So when you are migrating to spark 2.0 you need to move your code to use the built in csv source rather than using third party one.&lt;/p&gt;

&lt;h2 id=&quot;migrating-to-new-connector&quot;&gt;Migrating to New Connector&lt;/h2&gt;

&lt;p&gt;The steps for migrating from old connector to new one are as below.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;removing-dependency&quot;&gt;Removing Dependency&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first step to migrate code is to remove spark-csv dependency from the build. This makes sure that it doesnâ€™t conflict with built in connector.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;migrating-code&quot;&gt;Migrating Code&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The below code snippets show the changes need to migrate code. Itâ€™s relatively small change as built-in connector preserves all the same options
that were available in spark-csv.&lt;/p&gt;

&lt;p&gt;If you have below code in spark 1.x&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDF&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;com.databricks.spark.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;header&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;../test_data/sales.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Migrate the code to spark 2.0 as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadedDF&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;option&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;header&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;../test_data/sales.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see from the code, you need to replace the source from &lt;em&gt;com.databricks.spark.csv&lt;/em&gt; to &lt;em&gt;csv&lt;/em&gt;. This will migrate your code to use built in spark connector.&lt;/p&gt;

&lt;p&gt;You can access complete code on github for &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration/blob/master/spark-one/src/main/scala/com/madhukaraphatak/spark/migration/sparkone/CsvLoad.scala&quot;&gt;1.x&lt;/a&gt; and &lt;a href=&quot;https://github.com/phatak-dev/spark-two-migration/blob/master/spark-two/src/main/scala/com/madhukaraphatak/spark/migration/sparktwo/CsvLoad.scala&quot;&gt;2.x&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;advantages-of-built-in-csv-connector&quot;&gt;Advantages of Built-in CSV Connector&lt;/h2&gt;

&lt;p&gt;Now, if both connector provides the same API, you may wonder whatâ€™s the advantage of the upgrading to built in source. The below are the some of the advantages.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;no-third-party-dependency&quot;&gt;No third party dependency&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As this connector is built in to spark, we donâ€™t have to depend upon any third party library jars. This is makes playing with csv much easier in spark-shell or any
other interactive tools. Also it simplifies the dependency graph of our projects.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;better-performance-in-schema-inference&quot;&gt;Better Performance in Schema Inference&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Csv connector has an option to do schema inference. As third party library, earlier connector was pretty slow to do the schema inference. But
now the connector is built in to the spark it can use some of the optimised internal APIâ€™s to do it much faster.&lt;/p&gt;

&lt;p&gt;The below is the comparison for schema inference on 700mb data with 29 columns.I am using airlines data for year 2007 from &lt;a href=&quot;http://stat-computing.org/dataexpo/2009/the-data.html&quot;&gt;here&lt;/a&gt;. Itâ€™s zipped. When you unzip, you get csv file on which the tests are done. Test is done on spark-shell with master as &lt;em&gt;local&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The results as below&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark-Csv connector   51s&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Built-in  connector   20s&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As you can see from the results, built-in connector is almost twice as fast as earlier one.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;support-for-broadcast-join&quot;&gt;Support For Broadcast Join&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Earlier spark-csv connector didnâ€™t support broadcast join. So joins are very slow when we combine big dataframes with small ones for csv data. But
now built-in connector supports the broadcast joins which vastly improves the performance of joins.&lt;/p&gt;

&lt;p&gt;So I have created another small file with first 10000 rows of 2007.csv which is around 1mb. When we join the data on &lt;em&gt;Year&lt;/em&gt; column using below code&lt;/p&gt;

&lt;h4 id=&quot;join-code&quot;&gt;Join code&lt;/h4&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;joinedDf&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;smalldf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Year&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;===&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;smalldf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Year&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;joinedDf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here &lt;em&gt;df&lt;/em&gt; dataframe on 700mb data and &lt;em&gt;smalldf&lt;/em&gt; on 1 mb. We are running count to force spark to do complete join.&lt;/p&gt;

&lt;p&gt;I observed below results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark-Csv connector 52 min&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Built-in Connector 4.1 min&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As you can see, there is huge difference between join performance. The difference comes as spark built-in connector uses BroadcastJoin where as spark-csv uses SortMergeJoin. So when you migrate to built-in connector you will observe a significant improvement in the join performance.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So whenever you are moving to spark 2.0, use built in csv connector. It preserves the same API  and gives better performance than older connector.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;Whatâ€™s Next?&lt;/h2&gt;

&lt;p&gt;In next blog, we will be discuss about migrating rdd based code in spark 2.0&lt;/p&gt;

</description>
        <pubDate>Sat, 15 Apr 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/migrating-to-spark-two-part-2</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/migrating-to-spark-two-part-2</guid>
      </item>
    
      <item>
        <title>Migrating to Spark 2.0 - Part 1 : Scala Version and Dependencies</title>
        <description>&lt;p&gt;Spark 2.0 brings a significant changes to abstractions and APIâ€™s of spark platform. With performance boost, this version has made some of non backward compatible changes to the framework. To keep up to date with the latest updates, one need to migrate their spark 1.x code base to 2.x. In last few weeks, I was involved in migrating one of fairly large code base and found it quite involving process. In this series of posts, I will be documenting my experience of migration so it may help all the ones out there who are planning to do the same.&lt;/p&gt;

&lt;p&gt;This is the first post in this series. In this post, we will discuss how to upgrade our dependencies to add right support for spark 2.0. You can access all the posts &lt;a href=&quot;/categories/spark-two-migration-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;choosing-right-scala-version&quot;&gt;Choosing Right Scala Version&lt;/h2&gt;

&lt;p&gt;When you want to upgrade from spark 1.x to spark 2.x, first task is to pick the right scala version. In spark 1.x, spark was built using scala version 2.10.6. But from spark 2.0, the default version is changed to 2.11.8. 2.10 version is still supported even though itâ€™s not default.&lt;/p&gt;

&lt;p&gt;Scala major versions are non binary compatible, i.e you cannot mix and match the libraries built using 2.10 and 2.11. So whenever you change the scala version of the project, you need to upgrade all the libraries of the project including non-spark ones. Itâ€™s a significant work as you need to comb through each and every dependency and make sure right version exist.&lt;/p&gt;

&lt;p&gt;Initially I started the upgrade using Scala 2.10 as it was least resistance path. All the other external libraries needed no change and it was smooth. But I soon realised the distribution at spark download page &lt;a href=&quot;https://spark.apache.org/downloads.html&quot;&gt;https://spark.apache.org/downloads.html&lt;/a&gt; is only built using scala 2.11. So to support 2.10 I have to build my own distribution. Also I came across the &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-19810&quot;&gt;jira&lt;/a&gt; which discusses about removing scala 2.10 support altogether in 2.3.0. So this meant investing in 2.10 will be not good as it will be obsolete in next few versions.&lt;/p&gt;

&lt;p&gt;So I chose &lt;em&gt;2.11.8&lt;/em&gt; as my scala version for upgrade.&lt;/p&gt;

&lt;h2 id=&quot;choosing-right-java-version&quot;&gt;Choosing Right Java Version&lt;/h2&gt;

&lt;p&gt;From Spark 2.1.0 version, support for Java 7 has been deprecated. So I started using Java 8 for building and deploying the code.&lt;/p&gt;

&lt;h2 id=&quot;updating-external-dependencies&quot;&gt;Updating External Dependencies&lt;/h2&gt;

&lt;p&gt;One of the major challenges of changing scala version is to update all the project dependencies. My project had a fair bit of them and luckily all of those libraries had scala 2.11 version. So please make sure that all the libraries have 2.11 version before you make decision to change scala version.&lt;/p&gt;

&lt;h2 id=&quot;updating-connectors&quot;&gt;Updating Connectors&lt;/h2&gt;

&lt;p&gt;There are major changes happened to the connector ecosystem in spark 2.0. So when you are upgrading to spark 2.0, you need to make sure that you use the right connectors.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;removal-of-built-in-streaming-connectors&quot;&gt;Removal of Built in Streaming Connectors&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Earlier spark had support for zeromq, twitter as part of spark streaming code base. But in spark 2.x, they have removed &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13843&quot;&gt;it&lt;/a&gt;. No more these connectors are part of spark-streaming. This is done mostly to develop these connectors independent of spark versions. So if you are using these connector code will break.&lt;/p&gt;

&lt;p&gt;To fix this issue, you need to update the dependencies to point to &lt;a href=&quot;https://github.com/apache/bahir&quot;&gt;Apache Bahir&lt;/a&gt;. Apache Bahir is new home to all of this deleted connectors. Follow the README of bahir repository to update the dependencies to bahir ones.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;spark-20-specific-connectors&quot;&gt;Spark 2.0 specific connectors&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many popular connectors now give spark 2.0 specific connectors to build with. These connectors provide both scala 2.10 and 2.11 version. Choose the right one depending upon the scala version you have chosen. As I have chosen 2.11, the below are the some of updated connectors for some sources&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;elastic-search&quot;&gt;Elastic Search&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Elastic search has a dedicated spark connector which was used to be called as elasticsearch-hadoop. You can access latest connector &lt;a href=&quot;https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11/5.3.0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;csv-connector&quot;&gt;Csv Connector&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From Spark 2.0, csv is built in source. Earlier we used to use &lt;a href=&quot;https://github.com/databricks/spark-csv&quot;&gt;spark-csv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you are using any other connector, make sure they support 2.0. One thing to note that, if the connector is available in right scala version, it doesnâ€™t need any code changes to support spark 2.x. Spark 2.x data source API is backward compatible with spark 1.x&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So by updating scala version, java version and using right connectors you can update your project build to use spark 2.x.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;Whatâ€™s Next?&lt;/h2&gt;

&lt;p&gt;In next blog, we will be discuss about major changes in spark csv connector.&lt;/p&gt;

</description>
        <pubDate>Thu, 13 Apr 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/migrating-to-spark-two-part-1</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/migrating-to-spark-two-part-1</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 7 : Dynamic Scaling and Namespaces</title>
        <description>&lt;p&gt;In our last post we created two node spark cluster using kubernetes. Once we have defined and created the cluster
we can easily scale up or scale down using kubernetes. This elastic nature of kubernetes makes easy to scale
the infrastructure as and when the demand increases rather than setting up everything upfront.&lt;/p&gt;

&lt;p&gt;In this seventh blog of the series, we will discuss how to scale the spark cluster on kubernetes.
You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-scaling&quot;&gt;Dynamic Scaling&lt;/h2&gt;

&lt;p&gt;When we discussed deployment abstraction in our previous blog, we talked about &lt;em&gt;replica&lt;/em&gt; factor. In deployment configuration, we can specify the number
of replications we need for a given pod. This number is set to 1 in our current spark worker deployment.&lt;/p&gt;

&lt;p&gt;One of the nice thing about deployment abstraction is, we can change replica size dynamically without changing configuration. This
allows us to scale our spark cluster dynamically.&lt;/p&gt;

&lt;h3 id=&quot;scale-up&quot;&gt;Scale Up&lt;/h3&gt;

&lt;p&gt;Run below command to scale up workers from 1 to 2.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl scale deployment spark-worker --replicas 2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command takes deployment name as parameters and number of replicas. 
You can check results using&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When you run the above command, kubernetes creates more pods using template specified in spark-worker. Whenever these
pods come up they automatically connect to spark-master and scales the cluster.&lt;/p&gt;

&lt;h3 id=&quot;scale-down&quot;&gt;Scale Down&lt;/h3&gt;

&lt;p&gt;We can not only increase the workers, we can also scale down by setting lower replica numbers.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl scale deployment spark-worker --replicas 1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When above command executes, kubernetes will kill one of the worker to reduce the replica count to 1.&lt;/p&gt;

&lt;p&gt;Kubernetes automatically manages all the service related changes. So whenever we scale workers spark will automatically scale.&lt;/p&gt;

&lt;h2 id=&quot;multiple-clusters&quot;&gt;Multiple Clusters&lt;/h2&gt;

&lt;p&gt;Till now, we have run single cluster. But sometime we may want to run multiple clusters on same kubernetes cluster. If we try to run
same configurations twice like below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-master.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You will get below error&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Error from server: error when creating &amp;quot;spark-master.yaml&amp;quot;: deployments.extensions &amp;quot;spark-master&amp;quot; already exists&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Kubernetes is rejecting the request as the spark-master named deployment is already exist. One of the way to solve this issue is to
duplicate the configurations with different name. But it will be tedious and difficult to maintain.&lt;/p&gt;

&lt;p&gt;Better way to solve this issue to use  namespace abstraction of kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;namespace-abstraction&quot;&gt;Namespace Abstraction&lt;/h3&gt;

&lt;p&gt;Kubernetes allows users to create multiple virtual clusters on single physical cluster. These are called as namespaces.&lt;/p&gt;

&lt;p&gt;Namespace abstraction is used for allowing multiple users to share the same physical cluster. This abstraction gives scopes for names. This makes us to have same named services in different namespace.&lt;/p&gt;

&lt;p&gt;By default our cluster is running in a namespace called &lt;em&gt;default&lt;/em&gt;. In next section, we will create another namespace where we can run one more single node cluster.&lt;/p&gt;

&lt;h3 id=&quot;creating-namespace&quot;&gt;Creating Namespace&lt;/h3&gt;

&lt;p&gt;In order to create new cluster, first we need to cluster new namespace. Run below command to create namespace called &lt;em&gt;cluster2&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create namespace cluster2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can list all the namespaces using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see the below  result&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;NAME          STATUS    AGE
cluster2      Active    16s
default       Active    81d
kube-system   Active    81d&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;kube-system&lt;/em&gt; is the namespace in which all the kubernetes related pods run.&lt;/p&gt;

&lt;h3 id=&quot;setting-context&quot;&gt;Setting Context&lt;/h3&gt;

&lt;p&gt;By default, kubectl points to default namespace. We should change it to point to other one to create pods in our namespace. We can do it using changing the context variable.&lt;/p&gt;

&lt;p&gt;Run below command to change the context&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CONTEXT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;kubectl config view &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; awk &lt;span class=&quot;s1&quot;&gt;&amp;#39;/current-context/ {print $2}&amp;#39;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;
kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-context &lt;span class=&quot;nv&quot;&gt;$CONTEXT&lt;/span&gt; --namespace&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the first step, we get &lt;em&gt;CONTEXT&lt;/em&gt; variable. In the next command, we set namespace to &lt;em&gt;cluster2&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;creating-cluster-in-namespace&quot;&gt;Creating cluster in Namespace&lt;/h3&gt;

&lt;p&gt;Once we set the context, we can use same commands to create cluster. Letâ€™s run below the command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f .&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now our second cluster is started. We can see all the pods across the namespaces using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po --all-namespaces&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see some result something like below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;NAMESPACE     NAME                           READY     STATUS    RESTARTS   AGE
cluster2      spark-master-498980536-bxda1   1/1       Running   0          1m
cluster2      spark-worker-91608803-p1mfe    1/1       Running   0          1m
default       spark-master-498980536-cfw97   1/1       Running   0          46m
default       spark-worker-91608803-7pwhv    1/1       Running   0          46m
kube-system   kube-addon-manager-minikube    1/1       Running   17         81d
kube-system   kube-dns-v20-s0yyp             3/3       Running   80         81d
kube-system   kubernetes-dashboard-rb46j     1/1       Running   17         81d&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can observe from the result, there are multiple spark-master running in different namespaces.&lt;/p&gt;

&lt;p&gt;So using the namespace abstraction of kubernetes we can create multiple spark clusters on same kubernetes cluster.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog we discussed how to scale our clusters using kubernetes deployment abstraction. Also we discussed how to use 
namespace abstraction to create multiple clusters.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;Whatâ€™s Next?&lt;/h2&gt;

&lt;p&gt;Whenever we run services on kubernetes we may want to restrict their resource usage. This allows better infrastructure planning
and monitoring. In next blog, we will discuss about resource management on kubernetes.&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Mar 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-7</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-7</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 6 : Building Spark 2.0 Two Node Cluster</title>
        <description>&lt;p&gt;In last post, we have built spark 2.0 docker image. As a next step we will be building two node spark standalone cluster using that image. In the context of of kubernetes,  node analogues to a container. So in the sixth blog of the series, we will be building two node cluster containing single master and single worker.You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR you can access all the source code on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;spark-master-deployment&quot;&gt;Spark Master Deployment&lt;/h3&gt;

&lt;p&gt;To start with we define our master using kubernetes deployment abstraction. As you can recall from &lt;a href=&quot;/scaling-spark-with-kubernetes-part-3&quot;&gt;earlier&lt;/a&gt; post, deployment abstraction is used for defining one or morepods. Even though we need single master in our cluster, we will use deployment abstraction over pod as it gives us more flexiblity.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-2.1.0-bin-hadoop2.6&lt;/span&gt; 
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;IfNotPresent&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
          &lt;span class=&quot;l-Scalar-Plain&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;TCP&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/bin/bash&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;-c&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;--&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;./start-master.sh&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;infinity&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above yaml configuration shows the configuration for the master. The noteworthy pieces are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;image - We are using the image we built in our last post. This is availble in local docker images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;imagePullPolicy - By default kubernetes tries to pull the image from remote servers like dockerhub. But as our image is only available locally, we need to tell to kubernetes not to pull from remote. &lt;em&gt;imagePullPolicy&lt;/em&gt; property of configuration allows to us to control that. In our example, we say &lt;em&gt;IfNotPresent&lt;/em&gt; , which means pull only if there is no local copy. As we already have built the image, it will be avaialble and kubernetes will not try to pull from remote.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ports - We are exposing port &lt;em&gt;7077&lt;/em&gt; on which spark master will listen.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;command - Command is the configuration which tells what command to run when container bootstraps. Here we are specifying it to run &lt;em&gt;start-master&lt;/em&gt; script&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-master.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-spark-master&quot;&gt;Starting Spark Master&lt;/h3&gt;

&lt;p&gt;Once we have our configuration ready, we can start the spark master pod using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-master.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;spark-master-service&quot;&gt;Spark Master Service&lt;/h3&gt;

&lt;p&gt;Once we have defined and ran the spark master, next step is to define the service for spark master. This service exposes the spark master on network and other workers can connect to it.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# the port that this service should serve on&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;webui&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;8080&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;8080&lt;/span&gt;
  &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7077&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-master&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above yaml configuration for spark master service. We are naming the our service also &lt;em&gt;spark-master&lt;/em&gt; which helps in resolving proper hosts on cluster.&lt;/p&gt;

&lt;p&gt;We are also exposing the additional port 8080 for accessing spark web ui.&lt;/p&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-master-service.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-spark-master-service&quot;&gt;Starting Spark Master Service&lt;/h3&gt;

&lt;p&gt;Once we have defined the master service, we can now start the service using below command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-master-service.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;spark-worker-configuration&quot;&gt;Spark Worker Configuration&lt;/h3&gt;

&lt;p&gt;Once we have our spark master and itâ€™s service started, we can define the worker configuration.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;l-Scalar-Plain&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
&lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;l-Scalar-Plain&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
    &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-2.1.0-bin-hadoop2.6&lt;/span&gt; 
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;imagePullPolicy&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;IfNotPresent&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;spark-worker&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;7078&lt;/span&gt;
          &lt;span class=&quot;l-Scalar-Plain&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;l-Scalar-Plain&quot;&gt;TCP&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/bin/bash&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;-c&amp;quot;&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;--&amp;quot;&lt;/span&gt;
        &lt;span class=&quot;l-Scalar-Plain&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;p-Indicator&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;p-Indicator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;./start-worker.sh&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;infinity&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we are building two node cluster, we will be running only single worker as of now. Most of the configuration are same as master other than command which starts the worker.&lt;/p&gt;

&lt;p&gt;You can access complete configuration on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/spark-worker.yaml&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;starting-worker&quot;&gt;Starting Worker&lt;/h3&gt;

&lt;p&gt;You can start worker deployment using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl create -f spark-worker.yaml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we have all services are ready&lt;/p&gt;

&lt;h3 id=&quot;verifying-the-setup&quot;&gt;Verifying the Setup&lt;/h3&gt;

&lt;p&gt;Run below command to verify that both spark master and spark worker deployments are started.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command should two pods running as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;NAME                            READY     STATUS    RESTARTS   AGE
spark-master-498980536-6ljcw    1/1       Running   0          15h
spark-worker-1887160080-nmpq5   1/1       Running   0          14h&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Please note that exact name of the pod will differ from machine to machine.&lt;/p&gt;

&lt;p&gt;Once we verified the pods, verify the service using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl describe svc spark-master&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above command should show result as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Name:                   spark-master
Namespace:              default
Labels:                 name=spark-master
Selector:               name=spark-master
Type:                   ClusterIP
IP:                     10.0.0.147
Port:                   webui   8080/TCP
Endpoints:              172.17.0.3:8080
Port:                   spark   7077/TCP
Endpoints:              172.17.0.3:7077
Session Affinity:       None&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If both of the commands ran successfully, then we have spark cluster running successfully.&lt;/p&gt;

&lt;h3 id=&quot;testing-our-spark-cluster&quot;&gt;Testing our spark cluster&lt;/h3&gt;

&lt;p&gt;We can test our spark deployment using observing web ui and running some commands from spark shell.&lt;/p&gt;

&lt;h4 id=&quot;accessing-web-ui&quot;&gt;Accessing Web UI&lt;/h4&gt;

&lt;p&gt;In our configuration of spark master, we have exposed the UI port 8080. Normally it will be only available within spark cluster. But using the port forwarding, we can access the port on our local machine.&lt;/p&gt;

&lt;p&gt;First letâ€™s see the pods running on cluster using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl get po&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It should show the below result&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;NAME                           READY     STATUS    RESTARTS   AGE
spark-master-498980536-kfgg8   1/1       Running   0          14m
spark-worker-91608803-l22pw    1/1       Running   0          56s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We should port forward from master pod. Run below command. The exact name of the pod will differ from machine to machine.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl port-forward spark-master-498980536-kfgg8 8080:8080&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Port-forward takes two parameters. One is the pod name and then port pair. In port pair the first port is container port and next one is local.&lt;/p&gt;

&lt;p&gt;Once port is forwarded, go to this link &lt;a href=&quot;http://localhost:8080&quot;&gt;http://localhost:8080&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You should see the below image&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/spark-ui-kube.png&quot; alt=&quot;spark-ui-kube&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;spark-shell&quot;&gt;Spark Shell&lt;/h4&gt;

&lt;p&gt;Once we have spark ui, we can test the spark from shell. Letâ€™s run the spark shell from master container.&lt;/p&gt;

&lt;p&gt;First we need to login to our master pod. Run below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -it spark-master-498980536-kfgg8 bash&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Start the spark shell using below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;/opt/spark/bin/spark-shell --master spark://spark-master:7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Run below command to run some spark code&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makeRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If the code runs successfully, then our cluster setup is working.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this blog, we have succesfully built two node spark cluster using kubernetes absttractions.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;Whatâ€™s Next?&lt;/h3&gt;

&lt;p&gt;Now we have defined our barebone cluster. In next blog, we will how to scale the cluster using kubernetes tools. Also we will discuss how to do resource management in the cluster.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-6</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-6</guid>
      </item>
    
      <item>
        <title>Scalable Spark Deployment using Kubernetes - Part 5 : Building Spark 2.0 Docker Image</title>
        <description>&lt;p&gt;In last few posts of our kubernetes series, we discussed about the various abstractions available in the framework. In next set of posts, we will be
building a spark cluster using those abstractions. As part of the cluster setup, we will discuss how to use various different configuration available
in kubernetes to achieve some of the import features of clustering. This is the fifth blog of the series, where we will discuss about building a spark
2.0 docker image for running spark stand alone cluster. You can access all the posts in the series &lt;a href=&quot;/categories/kubernetes-series&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR you can access all the source code on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;need-for-custom-spark-image&quot;&gt;Need for Custom Spark Image&lt;/h3&gt;

&lt;p&gt;Kubernetes already has documented creating a spark cluster on &lt;a href=&quot;https://github.com/kubernetes/kubernetes/tree/master/examples/spark&quot;&gt;github&lt;/a&gt;. But currently it uses old version of the spark. Also it has some configurations which are specific to google cloud. These configurations are not often needed in most of the use cases. So in this blog, we will developing a simple spark image which is based on kubernetes one.&lt;/p&gt;

&lt;p&gt;This spark image is built for standalone spark clusters. From my personal experience, spark standalone mode is more suited for containerization
compared to yarn or mesos.&lt;/p&gt;

&lt;h3 id=&quot;docker-file&quot;&gt;Docker File&lt;/h3&gt;

&lt;p&gt;First step of creating a docker image is to write a docker file. In this section, we will discuss how to write a docker file needed
for spark.&lt;/p&gt;

&lt;p&gt;The below are the different steps of docker file.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Base Image&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;FROM java:openjdk-8-jdk&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above statement in the docker file defines the base image. We are using
a base image which gives us a debian kernel with java installed. We need 
java for all spark services.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Define Spark Version&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ENV spark_ver 2.1.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above line defines the version of spark. Using ENV, we can defines a variable and use it in different places in the script. Here we are building the spark with version 2.1.0. If you want other version, change this configuration.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download and Install Spark Binary&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;RUN mkdir -p /opt &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    curl http://www.us.apache.org/dist/spark/spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;-bin-hadoop2.6.tgz &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
        tar -zx &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    ln -s spark-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;-bin-hadoop2.6 spark &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Spark &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;spark_ver&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; installed in /opt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above curl command and downloads the spark binary. It will be symlinked into /opt/spark.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add start scripts to image&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;ADD start-common.sh start-worker.sh start-master.sh /
RUN chmod +x /start-common.sh /start-master.sh /start-worker.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above lines add some start scripts. We discuss more about these scripts
in next section.&lt;/p&gt;

&lt;p&gt;Now we have our docker file ready. Save it as &lt;em&gt;Dockerfile&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;You can access the complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/Dockerfile&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;scripts&quot;&gt;Scripts&lt;/h3&gt;

&lt;p&gt;In above, we have added some scripts for starting master and worker. Letâ€™s see whatâ€™s inside them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-common.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a script which runs before starting master and worker.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;unset &lt;/span&gt;SPARK_MASTER_PORT&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above script unsets a variable set by kubernetes. This is needed as this configuration interferes with the
spark clustering. We will discuss more about service variable in next post.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-common.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-master.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a script for starting master.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;

. /start-common.sh

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;$(hostname -i) spark-master&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; /etc/hosts

/opt/spark/sbin/start-master.sh --ip spark-master --port 7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the first step, we run the common script. We will be using &lt;em&gt;spark-master&lt;/em&gt; as the host name for our master container. So we are adding that into &lt;em&gt;/etc/hosts&lt;/em&gt; file.&lt;/p&gt;

&lt;p&gt;Then we start the master using &lt;em&gt;start-master.sh&lt;/em&gt; command. We will be listening on 7077 port for the master.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-master.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;start-worker.sh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the script for starting worker containers.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;

. /start-common.sh

/opt/spark/sbin/start-slave.sh spark://spark-master:7077&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It is similar to master script. The only difference is we are using &lt;em&gt;start-slave.sh&lt;/em&gt; for starting our worker nodes.&lt;/p&gt;

&lt;p&gt;You can access complete script on &lt;a href=&quot;https://github.com/phatak-dev/kubernetes-spark/blob/master/docker/start-worker.sh&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now we have our docker script ready. To build an image from the script, we need docker.&lt;/p&gt;

&lt;h3 id=&quot;installing-docker&quot;&gt;Installing Docker&lt;/h3&gt;

&lt;p&gt;You can install the docker on you machine using the steps &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;here&lt;/a&gt;. I am using docker version &lt;em&gt;1.10.0&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;using-kubernetes-docker-environment&quot;&gt;Using Kubernetes Docker Environment&lt;/h3&gt;

&lt;p&gt;Whenever we want to use docker, it normally runs a daemon on our machine. This daemon is used for building and pulling docker images. Even though we can build our docker image in our machine, it will be not that useful as our kubernetes runs in a vm. In this case, we need to push our docker image to vm and then only we can use the image in kubernetes.&lt;/p&gt;

&lt;p&gt;Alternative to that, another approach is to use minikube docker daemon. In this way we can build the docker images directly on our virtual machine.&lt;/p&gt;

&lt;p&gt;To access minikube docker daemon, run the below command&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;minikube docker-env&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can run&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;docker ps&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can see all the kubernetes containers as docker containers. Now you have successfully connected to minikube docker environment.&lt;/p&gt;

&lt;h3 id=&quot;building-image&quot;&gt;Building image&lt;/h3&gt;

&lt;p&gt;Clone code from github as below&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;git clone https://github.com/phatak-dev/kubernetes-spark.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;cd to &lt;em&gt;docker&lt;/em&gt; folder then run the below docker command.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;docker

docker build -t spark-2.1.0-bin-hadoop2.6 .&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In above command, we are tagging (naming) the image as &lt;em&gt;spark-2.1.0-bin-hadoop-2.6&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now our image is ready to deploy, spark 2.1.0 on kubernetes.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this post, we discussed how to build a spark 2.0 docker image from scratch. Having our own image gives more flexibility than using
off the shelf ones.&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;Whatâ€™s Next?&lt;/h3&gt;

&lt;p&gt;Now we have our spark image ready. In our next blog, we will discuss how to use this image to create a two node cluster in kubernetes.&lt;/p&gt;
</description>
        <pubDate>Sun, 26 Feb 2017 00:00:00 +0530</pubDate>
        <link>http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-5</link>
        <guid isPermaLink="true">http://blog.madhukaraphatak.com/scaling-spark-with-kubernetes-part-5</guid>
      </item>
    
  </channel>
</rss>
